{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Credit Risk and Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVNblDBpVoAq"
   },
   "source": [
    "**Names of all group members:**\n",
    "- Matthias Wyss (matthias.wyss@epfl.ch)\n",
    "- William Jallot (william.jallot@epfl.ch)\n",
    "- Antoine Garin (antoine.garin@epfl.ch)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "All code below is only suggestive and you may as well use different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1qPukjnSRgc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical means (training data): [48.7426539   7.98652219  0.1017    ]\n",
      "Empirical stds  (training data): [18.00788849  4.03090363  0.30226094]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1.\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)  # for reproducibility\n",
    "\n",
    "# simulate explanatory variables x\n",
    "m, n = 20000, 10000  # training and test sizes\n",
    "total = m + n\n",
    "\n",
    "# x1: age (18-80)\n",
    "x1 = np.random.uniform(18, 80, size=total)\n",
    "\n",
    "# x2: monthly income in kCHF (1-15)\n",
    "x2 = np.random.uniform(1, 15, size=total)\n",
    "\n",
    "# x3: employment status (0 = salaried, 1 = self-employed)\n",
    "x3 = np.random.choice([0, 1], size=total, p=[0.9, 0.1])\n",
    "\n",
    "# stack into a feature matrix\n",
    "X = np.column_stack((x1, x2, x3))\n",
    "\n",
    "# a) calculate empirical means and standard deviations over training data\n",
    "X_train = X[:m]  # first m samples as training data\n",
    "\n",
    "means = X_train.mean(axis=0)\n",
    "stds = X_train.std(axis=0, ddof=1)  # use ddof=1 for sample std\n",
    "\n",
    "print(\"Empirical means (training data):\", means)\n",
    "print(\"Empirical stds  (training data):\", stds)\n",
    "\n",
    "\n",
    "# b) Suggest other variables that would realistically be relevant in credit scoring.\n",
    "# (you do not have to implement those of course, just explain your answer in writing)\n",
    "\"\"\"\n",
    "Other variables that could be relevant for credit scoring include:\n",
    "- Credit history: past defaults, number of open loans, payment history.\n",
    "- Debt-to-income ratio: proportion of income already committed to debt payments.\n",
    "- Employment stability: length of current job, number of job changes.\n",
    "- Marital status / dependents: may affect financial obligations.\n",
    "- Education level: can correlate with income stability.\n",
    "- Age brackets or life stage: young vs. near retirement may carry different risk.\n",
    "- Housing situation: renter, owner, mortgage payments.\n",
    "- Other financial indicators: savings, assets, or investments.\n",
    "These features help better capture the borrower's ability and likelihood to repay.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhR3TshATJOo"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.\n",
    "# Building the datasets:\n",
    "\n",
    "sigmoid = lambda x: 1. / (1. + np.exp(-x))\n",
    "\n",
    "# build the first dataset\n",
    "\n",
    "\n",
    "# build the second dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rz30ywUHUOSG"
   },
   "outputs": [],
   "source": [
    "# Exercise 2. a)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "# \"model = LogisticRegression().fit(X_data, Y_data)\" fits a model\n",
    "# \"pred_X = model.predict_proba(X)\" evaluates the model\n",
    "# (note that it outputs both P(Y=0|X) and P(Y=1|X))\n",
    "# \"log_loss(Y, pred_X)\" evaluates the negative conditional log likelihood (also called cross-entropy loss)\n",
    "\n",
    "# Fit the models on both datasets\n",
    "\n",
    "\n",
    "# Calculate cross-entropy loss on both datasets for train and test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJawLeRKWIQJ"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.b)\n",
    "# Calculate normalized data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaQx76yCWOa3"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.b)\n",
    "from sklearn.svm import SVC\n",
    "# \"model = SVC(kernel='rbf', gamma=GAMMA, C=C, probability=True)\" creates\n",
    "# a model with kernel exp(-GAMMA \\|x-x'\\|_2^2) and regul. parameter C (note the relation between C and the parameter lambda).\n",
    "# \"probability=True\" enables the option \"model.predict_proba(X)\" to predict probabilities from the regression function \\hat{f}^{svm}.\n",
    "# \"model.fit(X, Y)\" optimizes the model parameters (using hinge loss)\n",
    "\n",
    "# Fit the models for both datasets (this can take up to 60 seconds with SVC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mS2tjfLdYO4Z"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.b)\n",
    "# \"model.predict_proba(X)\" predicts probabilities from features (note that it outputs both P(Y=0|X) and P(Y=1|X))\n",
    "\n",
    "# Calculate cross-entropy loss on both datasets for train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xTDD8SNZhZz"
   },
   "outputs": [],
   "source": [
    "# Exercise 2.c)\n",
    "import matplotlib.pyplot as plt\n",
    "# To calculate the curves, it is fine to take 100 threshold values c, i.e.,\n",
    "ths = np.linspace(0, 1, 100)\n",
    "\n",
    "# To approximately calculate the AUC, it is fine to simply use Riemann sums.\n",
    "# This means, if you have 100 (a_i, b_i) pairs for the curves, a_1 <= a_2 <= ...\n",
    "# then you may simply use the sum\n",
    "# sum_{i=1}^99 (b_i + b_{i+1})/2 * (a_{i+1}-a_i)\n",
    "# as the approximation of the integral (or AUC)\n",
    "\n",
    "\n",
    "# first data set & logistic regression:\n",
    "# (the code should be reusable for all cases, only exchanging datasets and predicted probabilities depending on the model)\n",
    "\n",
    "# Compute and plot the ROC and AUC cruves\n",
    "\n",
    "\n",
    "# second data set & logistic regression:\n",
    "\n",
    "\n",
    "# first data set and SVM:\n",
    "\n",
    "\n",
    "# second data set and SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YvGSeDoc9ZS"
   },
   "outputs": [],
   "source": [
    "# Exercise 3.\n",
    "\n",
    "# Set model parameters and define matrix D\n",
    "\n",
    "\n",
    "# Scenario 1:\n",
    "# Define Portfolio and possible outcomes for this portfolio using matrix D\n",
    "\n",
    "\n",
    "# Plot the histogram of profits and losses\n",
    "\n",
    "\n",
    "# Calculate expected profit and losses, compute 95%-VaR and 95%-ES\n",
    "\n",
    "\n",
    "# Scenario 2:\n",
    "# Define Portfolio and possible outcomes using the matrix D and the predicted default probabilities from the logistic regression model\n",
    "\n",
    "\n",
    "# Plot the histogram of profits and losses\n",
    "\n",
    "\n",
    "# Calculate expected profit and losses, compute 95%-VaR and 95%-ES\n",
    "\n",
    "\n",
    "# Scenario 3:\n",
    "# Define Portfolio and possible outcomes using the matrix D and the predicted default probabilities from the SVM model\n",
    "\n",
    "\n",
    "# Plot the histogram of profits and losses\n",
    "\n",
    "\n",
    "# Calculate expected profit and losses, compute 95%-VaR and 95%-ES"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (qrm)",
   "language": "python",
   "name": "qrm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
